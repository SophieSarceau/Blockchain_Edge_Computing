import gym
import torch as T
import torch.multiprocessing as mp
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical
import numpy as np
from torch.autograd import Variable
import random
import math
import hashlib

class BasicEnv(gym.Env):
    def __init__(self):
        self.action_space = gym.spaces.Discrete(360)
        self.observation_space = gym.spaces.Discrete(64^6554)
        self.current_state = np.array([2,1,2,3,2,2,5,4,7,3]) #  [GHz/bit , buffer1 ,GPUCycle1 ,buffer2 ,GPUCycle2, buffer3, GPUCycl3, buffer4, GPUCycle4, D(difficulty)
    def step(self, action):
        
        info = {}
        done = False
        # 动作转变
        action_data = [36,28,21,15,10,6,3,1]
        action_trans = 0
        action_blockchain = action // 120
        action_e = []
        if action_blockchain == 0:
            action_e = [2,3]
        elif action_blockchain == 1:
            action_e = [3,4]
        elif action_blockchain == 2:
            action_e = [1,2]
        action_a = action % 120
        action_a = action_a + 1
        server1 = 0
        for i in range(8):
            total_count = sum(action_data[0:i+1])
            if total_count >= action_a:
                server1 = i
                action_a = action_a - sum(action_data[0:i])
                break
        server2 = 0
        for i in range(8-server1):
            count_list = range(8-i)
            count_list = [number+1 for number in count_list]
            total_count = sum(count_list[-(i+1):])
            if total_count >= action_a:
                server2 = i
                count_list.reverse()
                action_a = action_a - sum(count_list[0:i])
                break
        server3 = action_a - 1
        action_trans = [server1, server2, server3, 7-(server1+server2+server3)] + action_e
        
        # task CPUCycle
        cpuCycle = self.current_state[0]
        
        # blcokchain task
        difficulty = self.current_state[9]
        average_hashcount = 1/((1/16)**difficulty)
        average_hashcount = int(average_hashcount)
        x=11
        y=1
        hashcount = 0
        hashstring = ''.join(random.sample('000',difficulty))
        while hashlib.sha256(f'{x*y}'.encode('utf-8')).hexdigest()[0:difficulty]!=hashstring:
            hashcount += 1
            y += 1
        
        # reward设置
        # 1 task time
        timelist = [0,0,0,0]
        bufferCPUlist = [self.current_state[2],self.current_state[4],self.current_state[6],self.current_state[8]]
        bufferlist = [self.current_state[1],self.current_state[3],self.current_state[5],self.current_state[7]]
        # 1-4服务器的算力发别为5，6，7，8
        serverCPU = [5,6,7,8]
        for i in range(len(timelist)):
            time_q = bufferlist[i]*8388608 * bufferCPUlist[i] / serverCPU[i] * (10**(-9))
            time_p = action_trans[i]*8388608 * cpuCycle / serverCPU[i] * (10**(-9))
            timelist[i] = time_q + time_p
        maxtime = max(timelist)
        # 2 mining time
        miningCPUCycle = 0
        if action_e == [2,3]:
            miningCPUCycle = 6+7
        elif action_e == [3,4]:
            miningCPUCycle = 7+8
        elif action_e == [1,2]:
            miningCPUCycle = 5+6
        miningtime = hashcount / miningCPUCycle * (10**(-4))
        # total time
        totaltime = maxtime + miningtime

        # bonus
        average_miningtime = average_hashcount / miningCPUCycle * (10**(-4))
        bonus = 0
        S = 1
        if miningtime < average_miningtime:
            bonus = 0.5*S
        elif miningtime >= average_miningtime:
            bonus = 0.5*S*math.exp(-(miningtime-average_miningtime))
        
        # cost
        u = [1,1.1,1.2,1.3]
        cost = 0
        for i in range(4):
            cost += action_trans[i]*8388608 * cpuCycle * (10**(-9)) * u[i]

        # reward(alpha beta gamma)
        alpha = 80
        beta = 10
        gamma = 10
        reward = -alpha*totaltime - beta*cost + gamma*bonus
        # 修改current_state
        #self.current_state = np.array([2,1,2,3,2,2,5,4,7,3]) #  [GHz/bit , buffer1 ,GPUCycle1 ,buffer2 ,GPUCycle2, buffer3, GPUCycl3, buffer4, GPUCycle4, D(difficulty)
        cpuCycle_value = [2,5,7]
        seed_task = random.random()
        taskcpuCycle = 0
        if seed_task < 0.2:
            taskcpuCycle = cpuCycle_value[0]
        elif (seed_task >= 0.2) and (seed_task < 0.8):
            taskcpuCycle = cpuCycle_value[1]
        elif seed_task > 0.8:
            taskcpuCycle = cpuCycle_value[2]
        
        taskbuffer_GPU = []
        for i in range(4):
            taskbuffer_GPU.append(random.randint(2,4))
            taskbuffer_GPU.append(cpuCycle_value[random.randint(1,3)-1])
        
        taskdifficulty = random.randint(2,3)
        state = [taskcpuCycle] + taskbuffer_GPU + [taskdifficulty]
        self.current_state = np.array(state)
        state = np.array(state)
        
        return state, reward, done, info, totaltime, cost
    def reset(self):
        
        state = np.array([2,1,2,3,2,2,5,4,7,3])
        self.current_state = state
        return state

class SharedAdam(T.optim.Adam):
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,
            weight_decay=0):
        super(SharedAdam, self).__init__(params, lr=lr, betas=betas, eps=eps,
                weight_decay=weight_decay)

        for group in self.param_groups:
            for p in group['params']:
                state = self.state[p]
                state['step'] = 0
                state['exp_avg'] = T.zeros_like(p.data)
                state['exp_avg_sq'] = T.zeros_like(p.data)

                state['exp_avg'].share_memory_()
                state['exp_avg_sq'].share_memory_()

class ActorCritic(nn.Module):
    def __init__(self, input_dims, n_actions, gamma=0.99):
        super(ActorCritic, self).__init__()

        self.gamma = gamma
        # 修改神经网络层数
        self.pi1 = nn.Linear(*input_dims, 1024)
        self.v1 = nn.Linear(*input_dims, 1024)
        self.pi2 = nn.Linear(1024, 1024)
        self.v2 = nn.Linear(1024, 1024)
        self.pi = nn.Linear(1024, n_actions)
        self.v = nn.Linear(1024, 1)

        self.rewards = []
        self.actions = []
        self.states = []

    def remember(self, state, action, reward):
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)

    def clear_memory(self):
        self.states = []
        self.actions = []
        self.rewards = []

    def forward(self, state):
        pi1 = F.relu(self.pi1(state))
        v1 = F.relu(self.v1(state))
        pi2 = F.relu(pi1)
        v2 = F.relu(v1)

        pi = self.pi(pi2)
        v = self.v(v2)

        return pi, v

    def calc_R(self, done):
        states = T.tensor(self.states, dtype=T.float)
        _, v = self.forward(states)

        R = v[-1]*(1-int(done))

        batch_return = []
        for reward in self.rewards[::-1]:
            R = reward + self.gamma*R
            batch_return.append(R)
        batch_return.reverse()
        batch_return = T.tensor(batch_return, dtype=T.float)

        return batch_return

    def calc_loss(self, done):
        states = T.tensor(self.states, dtype=T.float)
        actions = T.tensor(self.actions, dtype=T.float)

        returns = self.calc_R(done)

        pi, values = self.forward(states)
        values = values.squeeze()
        critic_loss = (returns-values)**2

        probs = T.softmax(pi, dim=1)
        dist = Categorical(probs)
        log_probs = dist.log_prob(actions)
        actor_loss = -log_probs*(returns-values)

        total_loss = (critic_loss + actor_loss).mean()
    
        return total_loss

    def choose_action(self, observation):
        state = T.tensor([observation], dtype=T.float)
        pi, v = self.forward(state)
        probs = T.softmax(pi, dim=1)
        dist = Categorical(probs)
        action = dist.sample().numpy()[0]

        return action

reward_list = []
for i in range(8):
    reward_list.append([])

class Agent(mp.Process):
    def __init__(self, global_actor_critic, optimizer, input_dims, n_actions, 
                gamma, lr, name, global_ep_idx, env_id):
        super(Agent, self).__init__()
        self.local_actor_critic = ActorCritic(input_dims, n_actions, gamma)
        self.global_actor_critic = global_actor_critic
        self.name = 'w%02i' % name
        self.episode_idx = global_ep_idx
        self.env = BasicEnv()       # 自己搭建custom环境后直接引用
        self.optimizer = optimizer

    def run(self):
        
        while self.episode_idx.value < 2000: # N_GAMES
            done = False
            observation = self.env.reset()         # 重点observation，貌似就是state
            score = 0
            total_time = 0
            total_cost = 0
            self.local_actor_critic.clear_memory()
            t_step = 1
            while t_step <= 125:
                action = self.local_actor_critic.choose_action(observation)
                observation_, reward, done, info, step_time, step_cost = self.env.step(action)      # step函数，重点搭建reward,因为env中记录了当前的state，所以只需要输入action即可
                score += reward
                total_time += step_time
                total_cost += step_cost
                self.local_actor_critic.remember(observation, action, reward)
                if t_step % 25 == 0:                 # T_MAX
                    loss = self.local_actor_critic.calc_loss(done)
                    self.optimizer.zero_grad()
                    loss.backward()
                    for local_param, global_param in zip(
                            self.local_actor_critic.parameters(),
                            self.global_actor_critic.parameters()):
                        global_param._grad = local_param.grad
                    # 更新学习率
                    self.optimizer.step()
                    self.local_actor_critic.load_state_dict(
                            self.global_actor_critic.state_dict())
                    self.local_actor_critic.clear_memory()
                t_step += 1
                observation = observation_
            with self.episode_idx.get_lock():
                self.episode_idx.value += 1
            score = score / 125
            total_time = total_time / 125
            total_cost = total_cost / 125
            # self.name, 'episode ', self.episode_idx.value
            #print('{', self.episode_idx.value, ',', '%.4f' % score, '},')
            print('{%.4f,%.4f,%.4f},'%(score,total_time,total_cost))
            reward_list[int(self.name[2])].append(score)

if __name__ == '__main__':
    lr = 1e-4
    env_id = 'CartPole-v0'
    n_actions = 360
    input_dims = [10]
    N_GAMES = 3000
    T_MAX = 20
    global_actor_critic = ActorCritic(input_dims, n_actions)
    global_actor_critic.share_memory()
    optim = SharedAdam(global_actor_critic.parameters(), lr=lr, 
                        betas=(0.92, 0.999))
    global_ep = mp.Value('i', 0)

    workers = [Agent(global_actor_critic,
                    optim,
                    input_dims,
                    n_actions,
                    gamma=0.99,
                    lr=lr,
                    name=i,
                    global_ep_idx=global_ep,
                    env_id=env_id) for i in range(mp.cpu_count())]
    [w.start() for w in workers]
    [w.join() for w in workers]
